# 🛠️ Using Framework Functions

## 1️⃣ Feature Collection

1. **Read contract addresses to a dataframe**
```python
addresses = get_Addresses(FileNames as list)
```
**Options:**
- **All files** → `["All"]`  
- **Specific file(s)** → `["file1.csv", "file2.csv"]`

   > The function reads files from the directory [`RawData/SC_Addresses/`](https://github.com/DIVE4Data/DIVE/tree/main/RawData/SC_Addresses) (To read files from a different directory, edit [`config.json`](https://github.com/DIVE4Data/DIVE/blob/main/config.json))

2. **Get contract features:**
```python
get_ContractFeatures(FeatureType as list, addresses as dataframe, DatasetName as a string,  session_path=None)
```
**Parameters:**
- **`FeatureTypes`** → List of feature types to extract. Supports:
   - `'AccountInfo'` or `'1'`  
   - `'ContractInfo'` or `'2'`  
   - `'Opcodes'` or `'3'`  
  > To extract **multiple types**, pass a list, e.g., `["AccountInfo", "Opcodes"]`, to extract **all supported types**, use `["All"]`
- **`addresses`** → A DataFrame containing contract addresses to be processed.
- **`datasetName`** → The name of the dataset that will be constructed.
- **`session_path`** → A path to a session file generated by the pipeline framework. Used to track intermediate or output file paths. This parameter is optional.

> 📁 Results are saved in: [`Features/API-based/`](https://github.com/DIVE4Data/DIVE/tree/main/Features/API-based) (To save in a different directory, edit [`config.json`](https://github.com/DIVE4Data/DIVE/blob/main/config.json))
---

## 2️⃣ Solidity Code Extraction
* This function extracts the source code for each contract and saves it as a `.sol` file. It also generates a summary file listing contract addresses and their compiler versions, which is useful in many cases, such as setting the correct `solc` version when running analysis tools. It is automatically invoked after fetching contract information, but can also be called manually
```python
extract_SourceCodes(ContractsInfo as a dataframe, UniqueFilename as a string, DatasetName as a string, session_path=None)
```
> 📁 Results are saved in: [`RawData/Samples/`](https://github.com/DIVE4Data/DIVE/tree/main/RawData/Samples) and [`RawData/SamplesSummary/`](https://github.com/DIVE4Data/DIVE/tree/main/RawData/SamplesSummary) (To save in different directories, edit [`config.json`](https://github.com/DIVE4Data/DIVE/blob/main/config.json))

---
## 3️⃣ Feature Extraction
```python
apply_FeatureExtraction(DatasetName as string,dataset_or_SamplesFolderName as DataFrame or path,attributes as list,session_path=None)
```
**Parameters:**
- **`datasetName`** → Name of the dataset from which features will be extracted.
- **`dataset_or_SamplesFolderName`** → Either a DataFrame containing attribute-level data or the name of a folder containing source code samples from which code metrics will be extracted.
- **`attributes`** →  Use `['all']` to extract features from all available attributes, or provide a list of specific attributes to extract features from. Supported values include:

   - `1` or  `'ABI'`
   - `2` or  `'Timestamp'`
   - `3` or  `'Library'`
   - `4` or  `'Transactionindex'`
   - `5` or  `'Code Metrics'`
   - `6` or  `'Input'` or `'Bytecode'`
   - `7` or  `'Opcode'`
  - **`session_path`** → Optional, it is created by the framework runner and used to track paths of files requested by methods.       
### 1. ABI Feature Extraction
```python
ABI_FeatureExtraction(DatasetName, dataset_or_SamplesFolderName, Col=attribute,session_path=None)
```
### 2. Timestamp Feature Extraction
```python
Timestamp_FeatureExtraction(DatasetName,dataset_or_SamplesFolderName, Col=attribute,session_path=None)
```
### 3. Library Feature Extraction
```python
library_FeatureExtraction(DatasetName,dataset_or_SamplesFolderName, Col=attribute, session_path=None)
```
### 4. TransactionIndex Feature Extraction
```python
 transactionIndex_FeatureExtraction(DatasetName,dataset_or_SamplesFolderName, Col=attribute, session_path=None)
```
### 5. Code Metrics Generation
```python
get_CodeMetrics(SamplesFolderName as a string, SamplesDirPath as a path, DatasetName as a string,session_path=None)
```
**Options:**

- To process samples from the **default directory**, set:  
  `SamplesDir = ""` or `SamplesDir = "default"`

- To process samples from a **custom directory**, set:  
  `SamplesDir = "path/to/your/samples"`
    
  > Default samples path: [`RawData/Samples`](https://github.com/SMART-DIVE/DIVE/tree/main/RawData/Samples)
  > 📁 Results are saved in: [`Features/FE-based/CodeMetrics/CodeMetrics/`](https://github.com/SMART-DIVE/DIVE/tree/main/Features/FE-based/CodeMetrics/CodeMetrics)
  > (To read or save in a different directory, edit [`config.json`](https://github.com/SMART-DIVE/DIVE/blob/main/config.json))

### 6. Bytecode/Input Feature Extraction
```python
Bytecode_FeatureExtraction(DatasetName,dataset_or_SamplesFolderName, Col=attribute,session_path=None)
```
### 7. Opcode Feature Extraction
```python
Opcode_FeatureExtraction(DatasetName,dataset_or_SamplesFolderName, Col=attribute, session_path=None)
```
---

## 4️⃣ Construct Final Data
```python
construct_FinalData(FinalDatasetName as a string, Dataset = ['Dataset1Name','Dataset2Name',...], FeatureTypes = {'Type1':['All' 'or list files'], 'Type2':['All' 'or list files'] , ... }, applyPreprocessing = False, session_path=None)
```
**Parameters:**
- **`FinalDatasetName`** → Name of the final dataset to be created.
- **`Dataset`** → List of dataset names to include, e.g., `['Dataset1Name', 'Dataset2Name']`.
- **`FeatureTypes`** → A dictionary specifying the feature types and corresponding files to include.  
  Available types include: `"AccountInfo"`, `"ContractsInfo"`, `"Opcodes"`, `"CodeMetrics"`, `"Opcode-based"`, `"Input-based"`, `"ABI-based"`, and `"Labels"`.  
  For each type, you can use:
  - `'all'` → to include all available files, or  
  - a list like `['File1.csv', 'File2.csv']` → to include specific files.
- **`applyPreprocessing`** → Set to `True` to apply preprocessing during construction, otherwise `False`.

> 📁 Data is collected from their default directories. Results are saved in: [`Datasets/`](https://github.com/SMART-DIVE/DIVE/tree/main/Datasets).  
> To use a different directory for saving or reading, edit [`config.json`](https://github.com/SMART-DIVE/DIVE/blob/main/config.json).

--- 

## 5️⃣ Apply Data Preprocessing

```python
apply_DataPreprocessing(datasetName as string, dataDirPath as bool or string, PreprocessingTasks as a list, session_path=None)
```

**Parameters:**
- **`datasetName`** → Name of the dataset.
- **`dataDirPath`** → `True` to use the default directory, or provide a custom path like `"path/to/your/samples"`.
- **`PreprocessingTasks`** → Use `['all']` to perform all tasks, or specify a list of tasks to apply. Supported tasks include:

  - `1` or `'DropDuplicates'`
  - `2` or `'HideProtectedAttributes'`
  - `3` or `'FillMissingData'`
  - `4` or `'ProcessConstructorArguments'`
  - `5` or `'ProcessOpcodes'`
  - `6` or `'RemoveUselesCols'`
  - `7` or `'HandlingHexaData'`
  - `8` or `'HandlingStringNumericalData'`
  - `9` or `'HandlingStringBoolColsToInt'`
  - `10` or `'HandlingCategoricalData'`
  - `11` or `'ConvertFloatColumns_to_Int'`
  - `12` or `'SetDataIndexColumn'`

> 📁 Results are saved in: [`Datasets/PreprocessedData/`](https://github.com/SMART-DIVE/DIVE/tree/main/Datasets/PreprocessedData)
> To use a different directory for saving or reading, edit [`config.json`](https://github.com/SMART-DIVE/DIVE/blob/main/config.json).
     
---

## 6️⃣ Statistical Data Generation

```python
get_DataStatistics(datasetName as string, rawDataName as string, voteDataName as string, QuickReport as bool)
```

The `get_DataStatistics` function generates statistical summaries and a profiling report for a given dataset. It performs:

- Basic dataset info and summary (`get_datasetInfo`, `get_datasetSummary`)
- Label distribution analysis (`get_LabelsFrequency`)
- Compiler version and timestamp frequency (`get_CompilerVersionsFrequency`, `get_TimestampFrequency`)
- Analysis tools frequency (`get_ToolsFrequency`), only if voting data is provided and contains a `Tools` column
- A profiling report using `pandas-profiling` (via the `ydata_profiling` package), with an option to generate a fast or detailed version (`get_ProfileReport`)

> **Note:** Although `get_DataStatistics()` runs all of these steps by default, you can also call each function individually if you only need specific parts of the analysis.

**Parameters:**
- **`datasetName`** → Name of the preprocessed dataset.
- **`rawDataName`** → Name of the original (unprocessed) dataset.
- **`voteDataName`** → Name of the dataset containing voting information (used when multiple analysis tools are applied for labeling). Otherwise, it can be left empty, i.e., `""`.
- **`QuickReport`** → Set to `True` to generate a minimal `pandas-profiling` report. If `False`, `ydata_profiling` will generate a full report, which may take longer on large datasets.

  
> 📁 Results are saved in: [`Statistics`](https://github.com/SMART-DIVE/DIVE/tree/main/Statistics) > To use a different directory for saving or reading, edit [`config.json`](https://github.com/SMART-DIVE/DIVE/blob/main/config.json).
---
